{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseUrl = 'https://www.nfl.com'\n",
    "teamAbbr = pd.Series([\"SF\",\"CHI\",\"CIN\",\"BUF\",\"DEN\",\"CLE\",\"TB\",\"ARI\",\"LAC\",\"KC\",\"IND\",\"DAL\",\"MIA\",\"PHI\",\"ATL\",\"NYG\",\n",
    "                     \"JAX\",\"NYJ\",\"DET\",\"GB\",\"CAR\",\"NE\",\"OAK\",\"LAR\",\"BAL\",\"WAS\",\"NO\",\"SEA\",\"PIT\",\"HOU\",\"TEN\",\"MIN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "playerPayload = {\"type\":\"/stats\",\"subject\":\"player-stats\",\"category\":\"category\",\n",
    "                 \"statisticCategory\":\"rushing\",\"season\":\"2019\",\n",
    "                 \"seasonType\":\"REG\",\n",
    "                 \"list\":\"all\",\n",
    "                 \"specificCategory\":\"rushingYards\",\n",
    "                 \"order\":\"DESC\"}\n",
    "\n",
    "teamPayload = {\"type\":\"/stats\",\"subject\":\"team-stats\",\"category\":\"offense\",\n",
    "               \"statisticCategory\":\"passing\",\n",
    "               \"season\":\"2019\",\n",
    "               \"seasonType\":\"REG\",\n",
    "               \"list\":\"all\",\n",
    "               \"specificCategory\":\"\",\n",
    "               \"order\":\"\"}\n",
    "\n",
    "schedulePayload = {\"type\":\"/schedules\",\n",
    "                  \"season\":\"2019\",\n",
    "                  \"seasonType\":\"REG\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = schedulePayload\n",
    "\n",
    "url = 'https://www.nfl.com'\n",
    "for key in payload:\n",
    "    if payload[key]:\n",
    "        url += payload[key]\n",
    "        url += '/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/schedules/2019/REG1/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/PRE0/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/PRE1/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/PRE2/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/PRE3/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/PRE4/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/REG1/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/REG2/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/REG3/\n",
      "Response: 200 https://www.nfl.com/schedules/2019/REG4/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-fd29f305cad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mseasonWeeks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mweek\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseasonWeeks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseUrl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mweek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Response:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "season = soup.find_all('select', {\"class\":\"d3-o-dropdown\"})[1].find_all(\"option\")\n",
    "seasonWeeks = []\n",
    "for week in season:\n",
    "    seasonWeeks.append(week.get(\"value\"))\n",
    "current = soup.find_all('select', {\"class\":\"d3-o-dropdown\"})[1].find('option', {\"selected\":True}).get(\"value\")\n",
    "print(current)\n",
    "seasonWeeks\n",
    "for week in seasonWeeks:\n",
    "    time.sleep(1)\n",
    "    response = requests.get(baseUrl+week)\n",
    "    print(\"Response:\", response.status_code,response.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.nfl.com/schedules/2019/REG/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-87c6972dfd40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "tableRows = []\n",
    "pagingText = True\n",
    "\n",
    "\n",
    "while pagingText:\n",
    "    time.sleep(2)\n",
    "    response = requests.get(url)\n",
    "    print(\"url:\",response.url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    table = soup.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if cols:\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            tableRows.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "    pagingText = soup.find(\"a\", {\"class\": \"nfl-o-table-pagination__next\"})\n",
    "    if pagingText:\n",
    "        url = baseUrl + pagingText.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDf = pd.DataFrame(tableRows, columns=columnHeader)\n",
    "# remove \\n whitespace from team names\n",
    "find = re.compile(r\"^(\\n*).*\")\n",
    "resultsDf['Team'] = resultsDf['Team'].apply(lambda x: re.search(find,x).group())\n",
    "pd.DataFrame.head(resultsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStats(url, payload): \n",
    "    tableRows = []\n",
    "    pagingText = True\n",
    "    \n",
    "    for key in payload:\n",
    "        if payload[key]:\n",
    "            url += payload[key]\n",
    "            url += '/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # get column names\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    columnHeader = []\n",
    "    header = soup.find('table').find_all('th')\n",
    "    for col in header:\n",
    "        columnHeader.append(col.text)\n",
    "    columnHeader = [c.strip('\\n') for c in columnHeader]\n",
    "\n",
    "    # search through pages and append to tableRows\n",
    "    while pagingText:\n",
    "        time.sleep(2)\n",
    "        response = requests.get(url)\n",
    "        print(\"url:\",response.url)\n",
    "        soup = BeautifulSoup(response.text,'html.parser')\n",
    "        table = soup.find('table')\n",
    "        rows = table.find_all('tr')\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                cols = [ele.text.strip() for ele in cols]\n",
    "                cols = [ele.rstrip('\\n') for ele in cols]\n",
    "                tableRows.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "        pagingText = soup.find(\"a\", {\"class\": \"nfl-o-table-pagination__next\"})\n",
    "        if pagingText:\n",
    "            url = baseUrl + pagingText.get('href')\n",
    "    \n",
    "    resultsDf = pd.DataFrame(tableRows, columns=columnHeader)\n",
    "    find = re.compile(r\"^(\\n*).*\")\n",
    "    resultsDf['Team'] = resultsDf['Team'].apply(lambda x: re.search(find,x).group())\n",
    "    \n",
    "    # add teamId to table\n",
    "    resultsDf = resultsDf.sort_values(by=['Team']).reset_index(drop=True)\n",
    "    resultsDf['teamId'] = teamAbbr\n",
    "    return resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teamPayload = {\"stats\":\"/stats\",\"subject\":\"team-stats\",\n",
    "               \"category\":\"offense\",\n",
    "               \"statisticCategory\":\"passing\",\n",
    "               \"season\":\"2019\",\n",
    "               \"seasonType\":\"REG\",\n",
    "               \"list\":\"all\",\n",
    "               \"specificCategory\":\"\",\n",
    "               \"order\":\"\"}\n",
    "df1 = getStats(baseUrl,teamPayload)\n",
    "pd.DataFrame.head(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatchups(url,payload):\n",
    "    for key in payload:\n",
    "        if payload[key]:\n",
    "            url += payload[key]\n",
    "            url += '/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    matchups = []\n",
    "    columnHeader = ['matchupId', 'team1', 'team2', 'team1Score','team2Score','weekType','weekNum']\n",
    "    season = soup.find_all('select', {\"class\":\"d3-o-dropdown\"})[1].find_all(\"option\")\n",
    "    seasonWeeks = []\n",
    "    for week in season:\n",
    "        seasonWeeks.append(week.get(\"value\"))\n",
    "    for week in seasonWeeks:\n",
    "        time.sleep(2)\n",
    "        url = baseUrl + week\n",
    "        response = requests.get(url)\n",
    "        print(url)\n",
    "        soup = BeautifulSoup(response.text,'html.parser')\n",
    "        weekName = week.rsplit('/', 2)[-2]\n",
    "        weekType = re.findall(\"[a-zA-Z]+\", weekName)[0]\n",
    "        weekNum = re.findall(r'\\d+', weekName)[0]\n",
    "        games = soup.find_all('a', {\"class\": \"nfl-c-matchup-strip__game\"})\n",
    "        index = 0\n",
    "        for game in games:\n",
    "            gameSoup = game.find_all('span', {\"class\":\"nfl-c-matchup-strip__team-abbreviation\"})\n",
    "            matchup = []\n",
    "            matchup.append(str(index))\n",
    "            for team in gameSoup:\n",
    "                matchup.append(re.sub(r'[^A-Za-z]', '', team.get_text()))\n",
    "            scoreSoup = game.find_all('div', {'class':\"nfl-c-matchup-strip__team-score\"})\n",
    "            if scoreSoup:\n",
    "                for team in scoreSoup:\n",
    "                    matchup.append(team.get_text())\n",
    "            else:\n",
    "                matchup.append(\"\")\n",
    "                matchup.append(\"\")\n",
    "            matchup.append(weekType)\n",
    "            matchup.append(weekNum)\n",
    "            matchups.append(matchup)\n",
    "            index += 1\n",
    "    matchupsDf = pd.DataFrame(matchups,columns=columnHeader)\n",
    "    return matchupsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nfl.com/schedules/2019/PRE0/\n",
      "https://www.nfl.com/schedules/2019/PRE1/\n",
      "https://www.nfl.com/schedules/2019/PRE2/\n",
      "https://www.nfl.com/schedules/2019/PRE3/\n",
      "https://www.nfl.com/schedules/2019/PRE4/\n",
      "https://www.nfl.com/schedules/2019/REG1/\n",
      "https://www.nfl.com/schedules/2019/REG2/\n",
      "https://www.nfl.com/schedules/2019/REG3/\n",
      "https://www.nfl.com/schedules/2019/REG4/\n",
      "https://www.nfl.com/schedules/2019/REG5/\n",
      "https://www.nfl.com/schedules/2019/REG6/\n",
      "https://www.nfl.com/schedules/2019/REG7/\n",
      "https://www.nfl.com/schedules/2019/REG8/\n",
      "https://www.nfl.com/schedules/2019/REG9/\n",
      "https://www.nfl.com/schedules/2019/REG10/\n",
      "https://www.nfl.com/schedules/2019/REG11/\n",
      "https://www.nfl.com/schedules/2019/REG12/\n",
      "https://www.nfl.com/schedules/2019/REG13/\n",
      "https://www.nfl.com/schedules/2019/REG14/\n",
      "https://www.nfl.com/schedules/2019/REG15/\n",
      "https://www.nfl.com/schedules/2019/REG16/\n",
      "https://www.nfl.com/schedules/2019/REG17/\n",
      "https://www.nfl.com/schedules/2019/POST1/\n",
      "https://www.nfl.com/schedules/2019/POST2/\n",
      "https://www.nfl.com/schedules/2019/POST3/\n",
      "https://www.nfl.com/schedules/2019/PRO1/\n",
      "https://www.nfl.com/schedules/2019/POST4/\n"
     ]
    }
   ],
   "source": [
    "schedulePayload = {\"type\":\"/schedules\",\n",
    "                  \"season\":\"2019\",\n",
    "                  \"seasonType\":\"REG\"}\n",
    "df2 = getMatchups(baseUrl,schedulePayload)\n",
    "\n",
    "#matchupsDf.loc[(matchupsDf['weekType']=='REG') & (matchupsDf['weekNum']=='1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matchupId</th>\n",
       "      <th>team1</th>\n",
       "      <th>team2</th>\n",
       "      <th>team1Score</th>\n",
       "      <th>team2Score</th>\n",
       "      <th>weekType</th>\n",
       "      <th>weekNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DEN</td>\n",
       "      <td>ATL</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>PRE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NYJ</td>\n",
       "      <td>NYG</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>PRE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>IND</td>\n",
       "      <td>BUF</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>PRE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>TEN</td>\n",
       "      <td>PHI</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>PRE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>ATL</td>\n",
       "      <td>MIA</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>PRE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>3</td>\n",
       "      <td>SEA</td>\n",
       "      <td>GB</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>POST</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0</td>\n",
       "      <td>TEN</td>\n",
       "      <td>KC</td>\n",
       "      <td>24</td>\n",
       "      <td>35</td>\n",
       "      <td>POST</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1</td>\n",
       "      <td>GB</td>\n",
       "      <td>SF</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>POST</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0</td>\n",
       "      <td>NFC</td>\n",
       "      <td>AFC</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "      <td>PRO</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0</td>\n",
       "      <td>SF</td>\n",
       "      <td>KC</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>POST</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     matchupId team1 team2 team1Score team2Score weekType weekNum\n",
       "0            0   DEN   ATL         14         10      PRE       0\n",
       "1            0   NYJ   NYG         22         31      PRE       1\n",
       "2            1   IND   BUF         16         24      PRE       1\n",
       "3            2   TEN   PHI         27         10      PRE       1\n",
       "4            3   ATL   MIA         27         34      PRE       1\n",
       "..         ...   ...   ...        ...        ...      ...     ...\n",
       "328          3   SEA    GB         23         28     POST       2\n",
       "329          0   TEN    KC         24         35     POST       3\n",
       "330          1    GB    SF         20         37     POST       3\n",
       "331          0   NFC   AFC         33         38      PRO       1\n",
       "332          0    SF    KC         20         31     POST       4\n",
       "\n",
       "[333 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "matchupsJson = df2.to_json(orient='records')\n",
    "\n",
    "with open(\"matchups.json\",\"w\") as outfile:\n",
    "    outfile.write(matchupsJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekTypeD = {\"PRE\":\"Pre-Season\", \"REG\":\"Regular Season\", \"POST\": \"Post-Season\"}\n",
    "\n",
    "\n",
    "def getWeeks(url,payload):\n",
    "    for key in payload:\n",
    "        if payload[key]:\n",
    "            url += payload[key]\n",
    "            url += '/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    columnHeader = ['weekType','weekNum','weekTypeF','weekF']\n",
    "    matchups = []\n",
    "    season = soup.find_all('select', {\"class\":\"d3-o-dropdown\"})[1].find_all(\"option\")\n",
    "    seasonWeeks = []\n",
    "    for week in season:\n",
    "        seasonWeeks.append(week.get(\"value\"))\n",
    "    for week in seasonWeeks:\n",
    "        url = baseUrl + week\n",
    "        weekName = week.rsplit('/', 2)[-2]\n",
    "        weekType = re.findall(\"[a-zA-Z]+\", weekName)[0]\n",
    "        weekNum = re.findall(r'\\d+', weekName)[0]\n",
    "        if weekType in weekTypeD.keys():\n",
    "            weekTypeF = weekTypeD[weekType]\n",
    "            weekF = weekTypeF + \" Week \" + weekNum\n",
    "            matchup = []\n",
    "            matchup.append(weekType)\n",
    "            matchup.append(weekNum)\n",
    "            matchup.append(weekTypeF)\n",
    "            matchup.append(weekF)\n",
    "            matchups.append(matchup)\n",
    "    weeksDf = pd.DataFrame(matchups,columns=columnHeader)\n",
    "    return weeksDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedulePayload = {\"type\":\"/schedules\",\n",
    "                  \"season\":\"2018\",\n",
    "                  \"seasonType\":\"REG\"}\n",
    "df3 = getWeeks(baseUrl,schedulePayload)\n",
    "df3\n",
    "weeksJson = df3.to_json(orient='records')\n",
    "\n",
    "with open(\"weeks.json\",\"w\") as outfile:\n",
    "    outfile.write(weeksJson)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
