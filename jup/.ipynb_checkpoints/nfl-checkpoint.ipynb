{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site used for scraping data\n",
    "baseUrl = 'https://www.nfl.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting stats off nfl.com/stats/\n",
    "def getStats(url, payload): \n",
    "    tableRows = []\n",
    "    pagingText = True\n",
    "    \n",
    "    for key in payload:\n",
    "        if payload[key]:\n",
    "            url += payload[key]\n",
    "            url += '/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # get column names\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    columnHeader = []\n",
    "    header = soup.find('table').find_all('th')\n",
    "    for col in header:\n",
    "        columnHeader.append(col.text)\n",
    "    columnHeader = [c.strip('\\n') for c in columnHeader]\n",
    "\n",
    "    # search through pages and append to tableRows\n",
    "    while pagingText:\n",
    "        time.sleep(2)\n",
    "        response = requests.get(url)\n",
    "        print(\"url:\",response.url)\n",
    "        soup = BeautifulSoup(response.text,'html.parser')\n",
    "        table = soup.find('table')\n",
    "        rows = table.find_all('tr')\n",
    "\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if cols:\n",
    "                cols = [ele.text.strip() for ele in cols]\n",
    "                cols = [ele.rstrip('\\n') for ele in cols]\n",
    "                tableRows.append([ele for ele in cols if ele]) # Get rid of empty values\n",
    "        pagingText = soup.find(\"a\", {\"class\": \"nfl-o-table-pagination__next\"})\n",
    "        if pagingText:\n",
    "            url = baseUrl + pagingText.get('href')\n",
    "    \n",
    "    resultsDf = pd.DataFrame(tableRows, columns=columnHeader)\n",
    "    find = re.compile(r\"^(\\n*).*\")\n",
    "    resultsDf['Team'] = resultsDf['Team'].apply(lambda x: re.search(find,x).group())\n",
    "    resultsDf = resultsDf.sort_values(by=['Team']).reset_index(drop=True)\n",
    "    return resultsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting team stats, calls getStats()\n",
    "def getTeamStats(url,side,categ,season,cols):\n",
    "    allCols = ['Team','weekType']+cols\n",
    "    seasonTypes = [\"PRE\",\"REG\",\"POST\"]\n",
    "    df = None\n",
    "    for st in seasonTypes:\n",
    "        teamPayload = {\"stats\":\"/stats\",\"subject\":\"team-stats\",\n",
    "               \"category\":side,\n",
    "               \"statisticCategory\":categ,\n",
    "               \"season\":season,\n",
    "               \"seasonType\":st,\n",
    "               \"list\":\"all\"}\n",
    "        tempDf = getStats(baseUrl,teamPayload)\n",
    "        tempDf['weekType']=st\n",
    "        tempDf = tempDf[allCols] \n",
    "        \n",
    "        # change raw stats to ordering\n",
    "        for i in cols:\n",
    "            colName = i + '(o)'\n",
    "            temp = pd.to_numeric(tempDf[i])\n",
    "            tempDf[colName] = temp\n",
    "            tempDf = tempDf.sort_values(by=i).reset_index(drop=True)\n",
    "            tempDf[colName] = range(len(tempDf.index),0,-1)\n",
    "            temp = tempDf[colName].astype(str)\n",
    "            tempDf[colName] = temp\n",
    "            \n",
    "        df = pd.concat([df,tempDf])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.nfl.com/stats/team-stats/offense/passing/2019/pre/all\n",
      "url: https://www.nfl.com/stats/team-stats/offense/passing/2019/reg/all\n",
      "url: https://www.nfl.com/stats/team-stats/offense/passing/2019/post/all\n",
      "url: https://www.nfl.com/stats/team-stats/offense/rushing/2019/pre/all\n",
      "url: https://www.nfl.com/stats/team-stats/offense/rushing/2019/reg/all\n",
      "url: https://www.nfl.com/stats/team-stats/offense/rushing/2019/post/all\n",
      "url: https://www.nfl.com/stats/team-stats/defense/passing/2019/pre/all\n",
      "url: https://www.nfl.com/stats/team-stats/defense/passing/2019/reg/all\n",
      "url: https://www.nfl.com/stats/team-stats/defense/passing/2019/post/all\n",
      "url: https://www.nfl.com/stats/team-stats/defense/rushing/2019/pre/all\n",
      "url: https://www.nfl.com/stats/team-stats/defense/rushing/2019/reg/all\n",
      "url: https://www.nfl.com/stats/team-stats/defense/rushing/2019/post/all\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# specific stats desired\n",
    "poStats = ['Pass Yds','Yds/Att','Cmp %','TD','INT']\n",
    "roStats = ['Rush Yds','YPC','TD','Rush FUM']\n",
    "pdStats = ['Yds','Yds/Att','Cmp %','TD','INT']\n",
    "rdStats = ['Rush Yds','YPC','TD','Rush FUM']\n",
    "\n",
    "# function calls to get team stats\n",
    "teamPODf = getTeamStats(baseUrl,\"offense\",\"passing\",\"2019\",poStats)\n",
    "teamRODf = getTeamStats(baseUrl,\"offense\",\"rushing\",\"2019\",roStats)\n",
    "teamPDDf = getTeamStats(baseUrl,\"defense\",\"passing\",\"2019\",pdStats)\n",
    "teamRDDf = getTeamStats(baseUrl,\"defense\",\"rushing\",\"2019\",rdStats)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data frames to json files for copying to server\n",
    "dfs = [teamPODf,teamRODf,teamPDDf,teamRDDf]\n",
    "for i in range(len(dfs)):\n",
    "    with open(\"teamStat\"+str(i),\"w\") as outfile:\n",
    "        outfile.write(dfs[i].to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for getting NFL schedule\n",
    "def getMatchups(url,payload):\n",
    "    for key in payload:\n",
    "        if payload[key]:\n",
    "            url += payload[key]\n",
    "            url += '/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    \n",
    "    matchups = []\n",
    "    columnHeader = ['matchupId', 'team1', 'team2', 'team1Score','team2Score','weekType','weekNum']\n",
    "    season = soup.find_all('select', {\"class\":\"d3-o-dropdown\"})[1].find_all(\"option\")\n",
    "    seasonWeeks = []\n",
    "    for week in season:\n",
    "        seasonWeeks.append(week.get(\"value\"))\n",
    "    for week in seasonWeeks:\n",
    "        time.sleep(2)\n",
    "        url = baseUrl + week\n",
    "        response = requests.get(url)\n",
    "        print(url)\n",
    "        soup = BeautifulSoup(response.text,'html.parser')\n",
    "        weekName = week.rsplit('/', 2)[-2]\n",
    "        weekType = re.findall(\"[a-zA-Z]+\", weekName)[0]\n",
    "        weekNum = re.findall(r'\\d+', weekName)[0]\n",
    "        games = soup.find_all('a', {\"class\": \"nfl-c-matchup-strip__game\"})\n",
    "        index = 0\n",
    "        for game in games:\n",
    "            gameSoup = game.find_all('span', {\"class\":\"nfl-c-matchup-strip__team-abbreviation\"})\n",
    "            matchup = []\n",
    "            matchup.append(str(index))\n",
    "            for team in gameSoup:\n",
    "                matchup.append(re.sub(r'[^A-Za-z]', '', team.get_text()))\n",
    "            scoreSoup = game.find_all('div', {'class':\"nfl-c-matchup-strip__team-score\"})\n",
    "            if scoreSoup:\n",
    "                for team in scoreSoup:\n",
    "                    matchup.append(team.get_text())\n",
    "            else:\n",
    "                matchup.append(\"\")\n",
    "                matchup.append(\"\")\n",
    "            matchup.append(weekType)\n",
    "            matchup.append(weekNum)\n",
    "            matchups.append(matchup)\n",
    "            index += 1\n",
    "    matchupsDf = pd.DataFrame(matchups,columns=columnHeader)\n",
    "    return matchupsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nfl.com/schedules/2019/PRE0/\n",
      "https://www.nfl.com/schedules/2019/PRE1/\n",
      "https://www.nfl.com/schedules/2019/PRE2/\n",
      "https://www.nfl.com/schedules/2019/PRE3/\n",
      "https://www.nfl.com/schedules/2019/PRE4/\n",
      "https://www.nfl.com/schedules/2019/REG1/\n",
      "https://www.nfl.com/schedules/2019/REG2/\n",
      "https://www.nfl.com/schedules/2019/REG3/\n",
      "https://www.nfl.com/schedules/2019/REG4/\n",
      "https://www.nfl.com/schedules/2019/REG5/\n",
      "https://www.nfl.com/schedules/2019/REG6/\n",
      "https://www.nfl.com/schedules/2019/REG7/\n",
      "https://www.nfl.com/schedules/2019/REG8/\n",
      "https://www.nfl.com/schedules/2019/REG9/\n",
      "https://www.nfl.com/schedules/2019/REG10/\n",
      "https://www.nfl.com/schedules/2019/REG11/\n",
      "https://www.nfl.com/schedules/2019/REG12/\n",
      "https://www.nfl.com/schedules/2019/REG13/\n",
      "https://www.nfl.com/schedules/2019/REG14/\n",
      "https://www.nfl.com/schedules/2019/REG15/\n",
      "https://www.nfl.com/schedules/2019/REG16/\n",
      "https://www.nfl.com/schedules/2019/REG17/\n",
      "https://www.nfl.com/schedules/2019/POST1/\n",
      "https://www.nfl.com/schedules/2019/POST2/\n",
      "https://www.nfl.com/schedules/2019/POST3/\n",
      "https://www.nfl.com/schedules/2019/PRO1/\n",
      "https://www.nfl.com/schedules/2019/POST4/\n"
     ]
    }
   ],
   "source": [
    "# function call for getting 2019 season's schedule\n",
    "schedulePayload = {\"type\":\"/schedules\",\n",
    "                  \"season\":\"2019\",\n",
    "                  \"seasonType\":\"REG\"}\n",
    "df2 = getMatchups(baseUrl,schedulePayload)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send schedule data frame to json file\n",
    "matchupsJson = df2.to_json(orient='records')\n",
    "\n",
    "with open(\"matchups.json\",\"w\") as outfile:\n",
    "    outfile.write(matchupsJson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to name different sections of the season\n",
    "weekTypeD = {\"PRE\":\"Pre-Season\", \"REG\":\"Regular Season\", \"POST\": \"Post-Season\"}\n",
    "\n",
    "# function to get the different weeks of the 2019 schedule\n",
    "def getWeeks(url,payload):\n",
    "    for key in payload:\n",
    "        if payload[key]:\n",
    "            url += payload[key]\n",
    "            url += '/'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    columnHeader = ['weekType','weekNum','weekTypeF','weekF']\n",
    "    matchups = []\n",
    "    season = soup.find_all('select', {\"class\":\"d3-o-dropdown\"})[1].find_all(\"option\")\n",
    "    seasonWeeks = []\n",
    "    for week in season:\n",
    "        seasonWeeks.append(week.get(\"value\"))\n",
    "    for week in seasonWeeks:\n",
    "        url = baseUrl + week\n",
    "        weekName = week.rsplit('/', 2)[-2]\n",
    "        weekType = re.findall(\"[a-zA-Z]+\", weekName)[0]\n",
    "        weekNum = re.findall(r'\\d+', weekName)[0]\n",
    "        if weekType in weekTypeD.keys():\n",
    "            weekTypeF = weekTypeD[weekType]\n",
    "            weekF = weekTypeF + \" Week \" + weekNum\n",
    "            matchup = []\n",
    "            matchup.append(weekType)\n",
    "            matchup.append(weekNum)\n",
    "            matchup.append(weekTypeF)\n",
    "            matchup.append(weekF)\n",
    "            matchups.append(matchup)\n",
    "    weeksDf = pd.DataFrame(matchups,columns=columnHeader)\n",
    "    return weeksDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function call to get weeks and send to json file\n",
    "schedulePayload = {\"type\":\"/schedules\",\n",
    "                  \"season\":\"2018\",\n",
    "                  \"seasonType\":\"REG\"}\n",
    "df3 = getWeeks(baseUrl,schedulePayload)\n",
    "df3\n",
    "weeksJson = df3.to_json(orient='records')\n",
    "\n",
    "with open(\"weeks.json\",\"w\") as outfile:\n",
    "    outfile.write(weeksJson)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
